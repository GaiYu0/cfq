{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "from operator import *\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import matplotlib.pylab as pl\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/data/yu_gai/cfq'\n",
    "output_dir = '/work/yu_gai/cfq'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CFQ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['complexityMeasures',\n",
       " 'expectedResponse',\n",
       " 'expectedResponseWithMids',\n",
       " 'question',\n",
       " 'questionPatternModEntities',\n",
       " 'questionWithBrackets',\n",
       " 'questionWithMids',\n",
       " 'ruleIds',\n",
       " 'ruleTree',\n",
       " 'sparql',\n",
       " 'sparqlPattern',\n",
       " 'sparqlPatternModEntities']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlCtx.read.parquet(f'{input_dir}/dataset.parquet/')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Were M2 and M5 executive produced by a British executive producer of M0 and executive produced by M3 and M4\n",
      "SELECT count(*) WHERE {\n",
      "?x0 ns:film.producer.films_executive_produced M0 .\n",
      "?x0 ns:people.person.nationality ns:m.07ssc .\n",
      "M2 ns:film.film.executive_produced_by ?x0 .\n",
      "M2 ns:film.film.executive_produced_by M3 .\n",
      "M2 ns:film.film.executive_produced_by M4 .\n",
      "M5 ns:film.film.executive_produced_by ?x0 .\n",
      "M5 ns:film.film.executive_produced_by M3 .\n",
      "M5 ns:film.film.executive_produced_by M4\n",
      "}\n",
      "\n",
      "Were M1 , M2 , and M3 influenced by a film producer\n",
      "SELECT count(*) WHERE {\n",
      "?x0 a ns:film.producer .\n",
      "M1 ns:influence.influence_node.influenced_by ?x0 .\n",
      "M2 ns:influence.influence_node.influenced_by ?x0 .\n",
      "M3 ns:influence.influence_node.influenced_by ?x0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "rs = df.rdd.map(lambda r: [r['questionPatternModEntities'], r['sparqlPatternModEntities']]).take(n)\n",
    "for i, [question, query] in enumerate(rs):\n",
    "    print(question)\n",
    "    print(query)\n",
    "    if i < n - 1:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARQL syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SELECT', 'FILTER', 'WHERE', 'DISTINCT']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(r'[A-Z]+')\n",
    "df.rdd.flatMap(lambda r: re.findall(p, r['sparql'])).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['SELECT count(*) WHERE {', 'SELECT DISTINCT ?x0 WHERE {'], ['}'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = df.rdd.map(lambda r: r['sparqlPatternModEntities'].split('\\n')).cache()\n",
    "rdd.map(lambda r: r[0]).distinct().collect(), rdd.map(lambda r: r[-1]).distinct().collect()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interrogatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "interrogatives = df.rdd.map(lambda r: (r['questionPatternModEntities'].split(' ')[0], r['sparqlPatternModEntities'].split('\\n')[0])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Which', 'SELECT DISTINCT ?x0 WHERE {'),\n",
       " ('Did', 'SELECT count(*) WHERE {'),\n",
       " ('Was', 'SELECT count(*) WHERE {'),\n",
       " ('Who', 'SELECT DISTINCT ?x0 WHERE {'),\n",
       " ('Were', 'SELECT count(*) WHERE {'),\n",
       " ('What', 'SELECT DISTINCT ?x0 WHERE {')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interrogatives.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Were': 19457,\n",
       "             'Was': 68063,\n",
       "             'Did': 43051,\n",
       "             'What': 56616,\n",
       "             'Which': 26466,\n",
       "             'Who': 25704})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interrogatives.map(lambda r: r[0]).countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARQL TRIPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triple syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples = rdd.flatMap(lambda r: [l for l in r[1 : -1] if 'FILTER' not in l]).distinct()\n",
    "p = re.compile(r'^([^ ]+) ([^ ]+) ([^ ]+)( .)?$')\n",
    "triples = triples.map(lambda r: re.findall(p, r)).cache()\n",
    "assert triples.map(lambda r: len(r) == 1).reduce(and_)\n",
    "triples = triples.map(lambda r: r[0])\n",
    "assert triples.map(lambda r: r[-1] in ['', ' .']).reduce(and_)  # conjunction only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq(rdd):\n",
    "    v, n = zip(*sorted(rdd.countByValue().items(), key=lambda x: x[1]))\n",
    "    return np.array(n) / sum(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = triples.map(lambda r: r[0]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['?x0',\n",
       " '?x1',\n",
       " '?x2',\n",
       " '?x3',\n",
       " '?x4',\n",
       " '?x5',\n",
       " 'M0',\n",
       " 'M1',\n",
       " 'M2',\n",
       " 'M3',\n",
       " 'M4',\n",
       " 'M5',\n",
       " 'M6',\n",
       " 'M7',\n",
       " 'M8',\n",
       " 'M9']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(subjects.distinct().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = triples.map(lambda r: r[1]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4119e0870677>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rels' is not defined"
     ]
    }
   ],
   "source": [
    "pl.plot(np.arange(len(rels)), freq(relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels = sorted(triples.map(lambda r: r[1]).distinct().collect())\n",
    "rels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['?x0',\n",
       " '?x1',\n",
       " '?x2',\n",
       " '?x3',\n",
       " '?x4',\n",
       " '?x5',\n",
       " 'M0',\n",
       " 'M1',\n",
       " 'M2',\n",
       " 'M3',\n",
       " 'M4',\n",
       " 'M5',\n",
       " 'M6',\n",
       " 'M7',\n",
       " 'M8',\n",
       " 'M9',\n",
       " 'ns:business.employer',\n",
       " 'ns:fictional_universe.fictional_character',\n",
       " 'ns:film.actor',\n",
       " 'ns:film.cinematographer',\n",
       " 'ns:film.director',\n",
       " 'ns:film.editor',\n",
       " 'ns:film.film',\n",
       " 'ns:film.film_art_director',\n",
       " 'ns:film.film_costumer_designer',\n",
       " 'ns:film.film_distributor',\n",
       " 'ns:film.producer',\n",
       " 'ns:film.production_company',\n",
       " 'ns:film.writer',\n",
       " 'ns:m.02zsn',\n",
       " 'ns:m.0345h',\n",
       " 'ns:m.03_3d',\n",
       " 'ns:m.03rjj',\n",
       " 'ns:m.059j2',\n",
       " 'ns:m.05zppz',\n",
       " 'ns:m.06mkj',\n",
       " 'ns:m.07ssc',\n",
       " 'ns:m.09c7w0',\n",
       " 'ns:m.0b90_r',\n",
       " 'ns:m.0d05w3',\n",
       " 'ns:m.0d060g',\n",
       " 'ns:m.0d0vqn',\n",
       " 'ns:m.0f8l9c',\n",
       " 'ns:people.person']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(triples.map(lambda r: r[2]).distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['?x0',\n",
       " '?x1',\n",
       " '?x2',\n",
       " '?x3',\n",
       " '?x4',\n",
       " '?x5',\n",
       " 'M0',\n",
       " 'M1',\n",
       " 'M2',\n",
       " 'M3',\n",
       " 'M4',\n",
       " 'M5',\n",
       " 'M6',\n",
       " 'M7',\n",
       " 'M8',\n",
       " 'M9',\n",
       " 'ns:m.02zsn',\n",
       " 'ns:m.0345h',\n",
       " 'ns:m.03_3d',\n",
       " 'ns:m.03rjj',\n",
       " 'ns:m.059j2',\n",
       " 'ns:m.05zppz',\n",
       " 'ns:m.06mkj',\n",
       " 'ns:m.07ssc',\n",
       " 'ns:m.09c7w0',\n",
       " 'ns:m.0b90_r',\n",
       " 'ns:m.0d05w3',\n",
       " 'ns:m.0d060g',\n",
       " 'ns:m.0d0vqn',\n",
       " 'ns:m.0f8l9c']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(triples.filter(lambda r: r[1] != 'a').map(lambda r: r[2]).distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M2', 'ns:film.film.directed_by', 'M1', ' .')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ns:people.person.gender', 'ns:people.person.nationality']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples.filter(lambda r: r[1] != 'a' and r[2].startswith('ns')).map(lambda r: r[1]).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Instance of\" triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ = triples.filter(lambda r: r[1] == 'a').distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['?x0',\n",
       " '?x1',\n",
       " '?x2',\n",
       " '?x3',\n",
       " '?x4',\n",
       " '?x5',\n",
       " 'M0',\n",
       " 'M1',\n",
       " 'M2',\n",
       " 'M3',\n",
       " 'M4',\n",
       " 'M5',\n",
       " 'M6']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(is_.map(lambda r: r[0]).distinct().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ns:business.employer',\n",
       " 'ns:fictional_universe.fictional_character',\n",
       " 'ns:film.actor',\n",
       " 'ns:film.cinematographer',\n",
       " 'ns:film.director',\n",
       " 'ns:film.editor',\n",
       " 'ns:film.film',\n",
       " 'ns:film.film_art_director',\n",
       " 'ns:film.film_costumer_designer',\n",
       " 'ns:film.film_distributor',\n",
       " 'ns:film.producer',\n",
       " 'ns:film.production_company',\n",
       " 'ns:film.writer',\n",
       " 'ns:people.person']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(is_.map(lambda r: r[2]).distinct().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARQL Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = rdd.flatMap(lambda r: [l for l in r if 'FILTER' in l]).distinct()\n",
    "p = re.compile(r'^FILTER \\( ([^ ]+) != ([^ ]+) \\)( .)?$')  # not equal only\n",
    "xs = filters.map(lambda r: re.findall(p, r)).cache()\n",
    "assert xs.map(lambda r: len(r) == 1).reduce(and_)\n",
    "assert xs.map(lambda r: r[0][-1] in ['', ' .']).reduce(and_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['?x0',\n",
       " '?x1',\n",
       " '?x2',\n",
       " '?x3',\n",
       " '?x4',\n",
       " 'M0',\n",
       " 'M1',\n",
       " 'M2',\n",
       " 'M3',\n",
       " 'M4',\n",
       " 'M5',\n",
       " 'M6',\n",
       " 'M7']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(xs.map(lambda r: r[0][0]).distinct().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['?x0',\n",
       " '?x1',\n",
       " '?x2',\n",
       " '?x3',\n",
       " '?x4',\n",
       " '?x5',\n",
       " 'M0',\n",
       " 'M1',\n",
       " 'M2',\n",
       " 'M3',\n",
       " 'M4',\n",
       " 'M5',\n",
       " 'M6',\n",
       " 'M7',\n",
       " 'M8']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(xs.map(lambda r: r[0][1]).distinct().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcd1 95743 11968 11968 239357\n",
      "mcd2 95743 11968 11968 239357\n",
      "mcd3 95743 11968 11968 239357\n",
      "query_complexity_split 100654 9512 9512 239357\n",
      "query_pattern_split 94600 12489 12589 239357\n",
      "question_complexity_split 98999 10339 10340 239357\n",
      "question_pattern_split 95654 12115 11909 239357\n",
      "random_split 95744 11967 11967 239357\n"
     ]
    }
   ],
   "source": [
    "split_ids = !ls {input_dir}/splits | grep json\n",
    "for split_id in [s.replace('.json', '') for s in split_ids]:\n",
    "    split = json.load(open(f'{input_dir}/splits/{split_id}.json'))\n",
    "    np.savez(f'{output_dir}/splits/{split_id}', **{k : np.array(v) for k, v in split.items()})\n",
    "    print(split_id, len(split['trainIdxs']), len(split['devIdxs']), len(split['testIdxs']), df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(r\"^([A-Za-z0-9,']+[ ]?)+$\")\n",
    "df.rdd.map(lambda r: re.match(p, r['questionPatternModEntities']).string).zip(df.rdd.map(lambda r: r['questionPatternModEntities'])).map(lambda r: r[0] == r[1]).reduce(and_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "get = lambda rdd, i: rdd.map(lambda r: r[i])\n",
    "collect = lambda rdd: np.array(rdd.collect())\n",
    "fcollect = lambda rdd: np.array(rdd.flatMap(lambda r: r).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP = '[SEP]'\n",
    "PAD = '[PAD]'\n",
    "def mapper(r):\n",
    "    toks = r['questionPatternModEntities'].split(' ')\n",
    "    entities = sorted(set(re.findall(r'M[0-0]', r['questionPatternModEntities'])))\n",
    "    variables = sorted(set(re.findall(r'\\?x[0-9]', r['sparqlPatternModEntities'])))\n",
    "    concepts = []\n",
    "    for line in r['sparqlPatternModEntities'].split('\\n')[1 : -1]:\n",
    "        if 'FILTER' not in line:\n",
    "            [[concept, *_]] = re.findall(r'^[^ ]+ [^ ]+ ([^ ]+)( .)?$', line)\n",
    "            if concept.startswith('ns:'):\n",
    "                concepts.append(concept)\n",
    "    concepts = sorted(set(concepts))\n",
    "    seq = toks + [SEP] + concepts + [SEP] + variables\n",
    "    isconcept = len(toks) * [False] + len(concepts) * [True] + len(variables) * [False]\n",
    "    isvariable = len(toks) * [False] + len(concepts) * [False] + len(variables) * [True]\n",
    "    return seq, isconcept, isvariable\n",
    "\n",
    "rdd = df.rdd.map(mapper).cache()\n",
    "seq_rdd, isconcept_rdd, isvariable_rdd = get(rdd, 0).cache(), get(rdd, 1), get(rdd, 2)\n",
    "idx2tok = seq_rdd.flatMap(lambda r: r).distinct().collect() + [PAD]\n",
    "tok2idx = dict(map(reversed, enumerate(idx2tok)))\n",
    "\n",
    "d['n_tok'] = collect(seq_rdd.map(len))\n",
    "d['seq'] = fcollect(seq_rdd.map(lambda r: [tok2idx[tok] for tok in r]))\n",
    "d['isconcept'], d['isvariable'] = fcollect(isconcept_rdd), fcollect(isvariable_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = sorted(tok for tok in idx2tok if re.match(r'^M[0-9]$', tok))\n",
    "concepts = sorted(tok for tok in idx2tok if tok.startswith('ns:'))\n",
    "variables = sorted(tok for tok in idx2tok if re.match(r'^\\?x[0-9]$', tok))\n",
    "sp_toks = sc.broadcast(set(entities + concepts + variables))\n",
    "def mapper(r):\n",
    "    d = defaultdict(list)\n",
    "    for i, tok in enumerate(r):\n",
    "        if tok in sp_toks.value:\n",
    "            d[tok].append(i)\n",
    "\n",
    "    n = len(d)\n",
    "    tok = [tok2idx[tok] for tok in sorted(d)]\n",
    "    n_idx = [len(d[k]) for k in sorted(d)]\n",
    "    idx = sum((d[k] for k in sorted(d)), [])\n",
    "\n",
    "    return n, tok, n_idx, idx\n",
    "\n",
    "rdd = seq_rdd.map(mapper).cache()\n",
    "d['n'] = collect(get(rdd, 0))\n",
    "d['tok'] = fcollect(get(rdd, 1))\n",
    "d['n_idx'] = fcollect(get(rdd, 2))\n",
    "d['idx'] = fcollect(get(rdd, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(r):\n",
    "    src, rel, dst = [], [], []\n",
    "    for line in r['sparqlPatternModEntities'].split('\\n')[1 : -1]:\n",
    "        if 'FILTER' in line:\n",
    "            [[src_, dst_, *_]] = re.findall(r'^FILTER \\( ([^ ]+) != ([^ ]+) \\)( .)?$', line)\n",
    "            src.append(src_)\n",
    "            rel.append('!=')\n",
    "            dst.append(dst_)\n",
    "        else:\n",
    "            [[src_, rel_, dst_, *_]] = re.findall(r'^([^ ]+) ([^ ]+) ([^ ]+)( .)?$', line)\n",
    "            src.append(src_)\n",
    "            rel.append(rel_)\n",
    "            dst.append(dst_)\n",
    "\n",
    "    u, inv = np.unique(src + dst, return_inverse=True)\n",
    "    src, dst = np.split(np.arange(len(u))[inv], 2)\n",
    "    return src, rel, dst\n",
    "\n",
    "rdd = df.rdd.map(mapper).cache()\n",
    "d['src'], d['dst'] = fcollect(get(rdd, 0)), fcollect(get(rdd, 2))\n",
    "rel_rdd = get(rdd, 1).cache()\n",
    "d['m'] = collect(rel_rdd.map(len))\n",
    "rel_rdd = rel_rdd.flatMap(lambda r: r).cache()\n",
    "idx2rel = sorted(rel_rdd.distinct().collect())\n",
    "rel2idx = {rel : idx for idx, rel in enumerate(idx2rel)}\n",
    "d['rel'] = collect(rel_rdd.map(lambda r: rel2idx[r]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([idx2tok, tok2idx], open(f'{output_dir}/vocab.pickle', 'wb'))\n",
    "pickle.dump([idx2rel, rel2idx], open(f'{output_dir}/rel-vocab.pickle', 'wb'))\n",
    "np.savez(f'{output_dir}/data', **d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "tok_rdd = df.rdd.map(lambda r: [tok2idx[tok] for tok in r['questionPatternModEntities'].split(' ')])\n",
    "d['seq'] = fcollect(tok_rdd)\n",
    "d['n_tok'] = collect(tok_rdd.map(len))\n",
    "d['n_var'] = collect(df.rdd.map(lambda r: len(set(re.findall(r'\\?x[0-9]', r['sparqlPatternModEntities'])))))\n",
    "np.savez(f'{output_dir}/nvar', **d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(r):\n",
    "    concepts = []\n",
    "    for line in r['sparqlPatternModEntities'].split('\\n')[1 : -1]:\n",
    "        if 'FILTER' not in line:\n",
    "            [[concept, *_]] = re.findall(r'^[^ ]+ [^ ]+ ([^ ]+)( .)?$', line)\n",
    "            if concept.startswith('ns:'):\n",
    "                concepts.append(concept)\n",
    "    return set(concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "tok_rdd = df.rdd.map(lambda r: [tok2idx[tok] for tok in r['questionPatternModEntities'].split(' ')])\n",
    "d['seq'] = fcollect(tok_rdd)\n",
    "d['n_tok'] = collect(tok_rdd.map(len))\n",
    "\n",
    "con = df.rdd.map(mapper).cache()\n",
    "d['n_con'] = collect(con.map(len))\n",
    "idx2con = sorted(con.flatMap(lambda r: r).distinct().collect())\n",
    "con2idx = {con : idx for idx, con in enumerate(idx2con)}\n",
    "d['con'] = fcollect(con.map(lambda r: sorted(con2idx[con] for con in r)))\n",
    "\n",
    "np.savez(f'{output_dir}/concept', **d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
