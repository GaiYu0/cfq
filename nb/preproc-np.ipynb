{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import *\n",
    "import json\n",
    "from operator import *\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import matplotlib.pylab as pl\n",
    "import nltk\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['complexityMeasures',\n",
       " 'expectedResponse',\n",
       " 'expectedResponseWithMids',\n",
       " 'index',\n",
       " 'question',\n",
       " 'questionPatternModEntities',\n",
       " 'questionTemplate',\n",
       " 'questionWithBrackets',\n",
       " 'questionWithMids',\n",
       " 'ruleIds',\n",
       " 'ruleTree',\n",
       " 'sparql',\n",
       " 'sparqlPattern',\n",
       " 'sparqlPatternModEntities']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir = '/data/yu_gai/cfq'\n",
    "output_dir = '/work/yu_gai/cfq/data/cfq'\n",
    "\n",
    "df = sqlCtx.read.parquet(f'{input_dir}/dataset.parquet').sort('index').persist()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcd1 95743 11968 11968 239357\n",
      "mcd2 95743 11968 11968 239357\n",
      "mcd3 95743 11968 11968 239357\n",
      "query_complexity_split 100654 9512 9512 239357\n",
      "query_pattern_split 94600 12489 12589 239357\n",
      "question_complexity_split 98999 10339 10340 239357\n",
      "question_pattern_split 95654 12115 11909 239357\n",
      "random_split 95744 11967 11967 239357\n"
     ]
    }
   ],
   "source": [
    "splits = {}\n",
    "split_ids = !ls {input_dir}/splits | grep json\n",
    "for split_id in [s.replace('.json', '') for s in split_ids]:\n",
    "    split = splits[split_id] = json.load(open(f'{input_dir}/splits/{split_id}.json'))\n",
    "    np.savez(f'{output_dir}/splits/{split_id}', **{k : np.array(v) for k, v in split.items()})\n",
    "    print(split_id, len(split['trainIdxs']), len(split['devIdxs']), len(split['testIdxs']), df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace(q):\n",
    "    for s in [\n",
    "        'art director',\n",
    "        'country of nationality',\n",
    "        'costume designer',\n",
    "        'executive producer',\n",
    "        'executive produce',\n",
    "        'executive produced',\n",
    "        'film director',\n",
    "        'film distributor',\n",
    "        'film editor',\n",
    "        'film producer',\n",
    "        'production company',\n",
    "    ]:\n",
    "        q = q.replace(s, s.replace(' ', ''))\n",
    "    return q\n",
    "\n",
    "df = df.withColumn('questionPatternModEntities', udf(replace, StringType())('questionPatternModEntities')).persist()\n",
    "df.rdd.map(lambda r: len(r['questionPatternModEntities'].split(' ')) == len(r['questionTemplate'].split(' '))).reduce(and_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "at = lambda i: (lambda x: x[i])\n",
    "k1 = lambda r: [r, 1]\n",
    "unique = lambda rdd: sorted(rdd.distinct().collect())\n",
    "count = lambda rdd: dict(rdd.map(k1).reduceByKey(add).collect())\n",
    "collect = lambda rdd: np.array(rdd.collect())\n",
    "flat_collect = lambda rdd: np.array(rdd.flatMap(lambda r: r).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP, NIL = '{SEP}', '{NIL}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2tok, tok2idx = pickle.load(open(f'{output_dir}/tok-vocab.pickle', 'rb'))\n",
    "idx2tag, tag2idx = pickle.load(open(f'{output_dir}/tag-vocab.pickle', 'rb'))\n",
    "idx2typ, typ2idx = pickle.load(open(f'{output_dir}/typ-vocab.pickle', 'rb'))\n",
    "idx2attr, _ = pickle.load(open(f'{output_dir}/attr-vocab.pickle', 'rb'))\n",
    "roles, _ = pickle.load(open(f'{output_dir}/role-vocab.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rel(line):\n",
    "    if 'FILTER' in line:\n",
    "        [[src, dst, *_]] = re.findall(r'^FILTER \\( ([^ ]+) != ([^ ]+) \\)( .)?$', line)\n",
    "        return src, '!=', dst\n",
    "    else:\n",
    "        [[src, typ, dst, *_]] = re.findall(r'^([^ ]+) ([^ ]+) ([^ ]+)( .)?$', line)\n",
    "        return src, typ, dst\n",
    "\n",
    "# r = '(?:%s)' % '|'.join(fr'\\[{role[1 : -1]}\\]' for role in roles)  # TODO\n",
    "# p = re.compile(fr'{r} and {r}|(?:{r} , )+and {r}')\n",
    "p = re.compile('|'.join(fr'{r} and {r}|(?:{r} , )+and {r}' for r in [fr'\\[{role[1 : -1]}\\]' for role in roles]))\n",
    "def grp_by_tag(tags):\n",
    "    lens = np.array(list(map(len, tags)))\n",
    "    ends = np.cumsum(lens) + np.arange(len(tags))\n",
    "    starts = ends - lens\n",
    "\n",
    "    t = ' '.join(tags)\n",
    "    homo = lambda s: sum(role in s for role in roles) == 1\n",
    "    matches = [m for m in re.finditer(p, t) if homo(m.group())]\n",
    "    if not matches:\n",
    "        grps = [[i] for i in range(len(tags))]\n",
    "        return grps\n",
    "    \n",
    "    m_start, m_end = zip(*([m.start(), m.end()] for m in matches))\n",
    "    hit = False\n",
    "    grps = []\n",
    "    for idx, [start, end] in enumerate(zip(starts, ends)):\n",
    "        if start in m_start:\n",
    "            hit = True\n",
    "            grps.append([])\n",
    "        if hit:\n",
    "            grps[-1].append(idx)\n",
    "        else:\n",
    "            grps.append([idx])\n",
    "        if end in m_end:\n",
    "            hit = False\n",
    "    \n",
    "    for start, end, grp in zip(m_start, m_end, (grp for grp in grps if len(grp) > 1)):\n",
    "        assert t[start : end] == ' '.join(tags[idx] for idx in grp)\n",
    "\n",
    "    return grps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mapper(r):\n",
    "    rels = list(map(find_rel, r['sparqlPatternModEntities'].split('\\n')[1 : -1]))\n",
    "    srcs, typs, dsts = zip(*rels)\n",
    "    ents = sorted({x for x in chain(srcs, dsts) if re.match('M\\d', x) or re.match('\\?x\\d', x)})\n",
    "\n",
    "    tail = [SEP] + sorted(ent for ent in ents if ent.startswith('?x')) + [NIL]\n",
    "    toks = r['questionPatternModEntities'].split(' ') + tail\n",
    "    tags = r['questionTemplate'].split(' ') + tail\n",
    "    grps = grp_by_tag(tags)\n",
    "\n",
    "    seq = [tag2idx[tags[idx]] for idx, *_ in grps]\n",
    "    mem = [[tok2idx[toks[grp[0]]]] if len(grp) == 1 else\n",
    "           [tok2idx[toks[idx]] for idx in grp if tags[idx] in roles] for grp in grps]\n",
    "#     print([[toks[idx] for idx in grp] for grp in grps])\n",
    "#     print([[tags[idx] for idx in grp] for grp in grps])\n",
    "#     print(mem)\n",
    "    \n",
    "    ent2grp = {}\n",
    "    for idx, tok in zip(chain(*(len(grp) * [idx] for idx, grp in enumerate(grps))), toks):\n",
    "        if tok in ents:\n",
    "            ent2grp[tok] = idx\n",
    "    idx2grp = sorted(set(ent2grp.values()))\n",
    "    ent2idx = {ent: idx2grp.index(ent2grp[ent]) for ent in ents}\n",
    "    _, idx2ent = zip(*sorted(ent2idx.items(), key=at(1)))\n",
    "    \n",
    "    # filters\n",
    "    filters = [[ent2idx[src], ent2idx[dst]] for src, typ, dst in rels if typ == '!=']\n",
    "    \n",
    "    # attributes\n",
    "    ent2attr = np.zeros([len(idx2grp), len(idx2attr)])\n",
    "    for src, _, dst in rels:\n",
    "        if dst in idx2attr:\n",
    "            ent2attr[ent2idx[src], idx2attr.index(dst)] = 1\n",
    "\n",
    "    # groundable relations\n",
    "    gr_rels = [[ent2idx[src], ent2idx[dst], idx2typ.index(typ)] for src, typ, dst in rels if typ in idx2typ]\n",
    "\n",
    "    return filters, ent2attr, gr_rels, seq, mem, idx2grp\n",
    "\n",
    "rdd = df.rdd.map(_mapper).cache()\n",
    "\n",
    "dat = {}\n",
    "dat['n_filter'] = collect(rdd.map(at(0)).map(len))\n",
    "dat['filter'] = collect(rdd.flatMap(at(0)))\n",
    "dat['attr'] = np.vstack(rdd.map(at(1)).collect())\n",
    "dat['n_rel'] = collect(rdd.map(at(2)).map(len))\n",
    "dat['src'] = collect(rdd.flatMap(at(2)).map(at(0)))\n",
    "dat['dst'] = collect(rdd.flatMap(at(2)).map(at(1)))\n",
    "dat['typ'] = collect(rdd.flatMap(at(2)).map(at(2)))\n",
    "dat['seq'] = collect(rdd.flatMap(at(3)))\n",
    "dat['n_grp'] = collect(rdd.map(at(4)).map(len))\n",
    "dat['n_mem'] = collect(rdd.flatMap(at(4)).map(len))\n",
    "dat['mem'] = collect(rdd.flatMap(at(4)).flatMap(lambda r: r))\n",
    "dat['n'] = collect(rdd.map(at(5)).map(len))\n",
    "dat['idx2grp'] = collect(rdd.flatMap(at(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " S  \n",
      " |   \n",
      " NP \n",
      " |   \n",
      "What\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP\n",
    "    DT -> 'a' | 'an'\n",
    "    JJ -> 'ADJECTIVE_SIMPLE'\n",
    "    N -> 'NP_SIMPLE' | 'ROLE_SIMPLE' | JJ N\n",
    "    NP -> 'entity' | DT N | 'What' | 'What' N | 'Which' N | 'Who' | NP POS N | NP PP NP | NP 'whose' N\n",
    "    POS -> \"'s\"\n",
    "    PP -> 'of'\n",
    "\"\"\")\n",
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "def find_noun_phrases(tags):\n",
    "    hits = []\n",
    "    start = 0\n",
    "    while start < len(tags):\n",
    "        for end in range(len(tags), start, -1):\n",
    "            try:\n",
    "                trees = list(parser.parse(tags[start : end]))\n",
    "            except ValueError:\n",
    "                trees = []\n",
    "            if len(trees) > 0:\n",
    "                hits.append([start, end, trees])\n",
    "                start = end - 1\n",
    "                break\n",
    "        start += 1\n",
    "    return hits\n",
    "            \n",
    "for start, end, trees in find_noun_phrases(\"What\".split(' ')):\n",
    "# for start, end, trees in find_noun_phrases(\"entity 's ADJECTIVE_SIMPLE ADJECTIVE_SIMPLE ADJECTIVE_SIMPLE ROLE_SIMPLE\".split(' ')):\n",
    "    for tree in trees:\n",
    "        tree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2str = lambda seq: ' '.join(idx2tag[idx] for idx in seq)\n",
    "str2seq = lambda r: [idx2tag.index(tag) for tag in r.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mapper(r):\n",
    "    idx2tag_ = [tag.replace('[', '').replace(']', '') for tag in idx2tag]\n",
    "    templ = [idx2tag_[idx] for idx in r]\n",
    "    hits = find_noun_phrases(templ)\n",
    "\n",
    "    idx = 0\n",
    "    starts, ends, _ = zip(*hits)\n",
    "    tags = []\n",
    "    while idx < len(templ):\n",
    "        if idx in starts:\n",
    "            tags.append(len(idx2tag_))\n",
    "            idx = ends[starts.index(idx)]\n",
    "        else:\n",
    "            tags.append(idx2tag_.index(templ[idx]))\n",
    "            idx += 1\n",
    "    \n",
    "    noun_phrases = [[idx2tag_.index(tag) for tag in templ[start : end]] for start, end, _ in hits]\n",
    "    positions = [idx for idx, tag in enumerate(tags) if tag == len(idx2tag_)]\n",
    "\n",
    "    return tags, noun_phrases, positions\n",
    "\n",
    "dat_np = {}\n",
    "rdd_np = rdd.map(at(3)).map(lambda r: list(takewhile(lambda idx: idx2tag[idx] != '{SEP}', r))).map(_mapper).cache()\n",
    "dat_np['len_tag'] = collect(rdd_np.map(at(0)).map(len))\n",
    "dat_np['seq_tag'] = collect(rdd_np.flatMap(at(0)))\n",
    "dat_np['len_np'] = collect(rdd_np.map(at(1)).map(len))\n",
    "rdd_np_ = rdd_np.flatMap(at(1)).cache()\n",
    "dat_np['len_noun'] = collect(rdd_np_.map(len))\n",
    "dat_np['seq_noun'] = flat_collect(rdd_np_)\n",
    "dat_np['pos_np'] = collect(rdd_np.flatMap(at(2)))\n",
    "# dat_np['n_var'] = collect(rdd.map(at(3)).map(lambda r: seq2str(r).count('?x')))\n",
    "n_var = lambda r: sum(1 for idx in r if idx2tag[idx].startswith('?x'))\n",
    "# n = lambda r: sum(1 for idx in r if idx2tag[idx] in ['[NP_SIMPLE]', '[ROLE_SIMPLE]', 'Who']) + (seq2str(r).startswith('What did') or seq2str(r).startswith('What was'))\n",
    "n = lambda r: sum(1 for idx in r if idx2tag[idx] in ['[NP_SIMPLE]', '[ROLE_SIMPLE]'])\n",
    "dat_np['n_var'] = collect(rdd.map(at(3)).map(lambda r: n_var(r) - n(r) + 1))\n",
    "np.savez(f'{output_dir}/var', **dat_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(dat_np['n_var'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mapper(r):\n",
    "    _, _, rels, seq, _, idx2grp = r\n",
    "    idx2tag_ = [tag.replace('[', '').replace(']', '') for tag in idx2tag]\n",
    "    templ = [idx2tag_[idx] for idx in seq]\n",
    "    hits = find_noun_phrases(templ)\n",
    "#     idx_sep, idx_nil = templ.index(SEP), templ.index(NIL)\n",
    "#     if idx_sep + 1 < idx_nil:\n",
    "#         hits.append([idx_sep + 1, idx_nil, None])\n",
    "\n",
    "    idx = idx_tag = 0\n",
    "    starts, ends, _ = zip(*hits)\n",
    "    tags = []\n",
    "    isnoun = []\n",
    "    pos_noun = []\n",
    "    pos_tag = []\n",
    "    while idx < len(templ):\n",
    "        if idx in starts:\n",
    "            tags.append(len(idx2tag_))\n",
    "            end = ends[starts.index(idx)]\n",
    "            k = end - idx\n",
    "            isnoun.extend(k * [True])\n",
    "            pos_noun.extend(range(k))\n",
    "            pos_tag.extend(k * [0])\n",
    "            idx = end\n",
    "        else:\n",
    "            tags.append(idx2tag_.index(templ[idx]))\n",
    "            isnoun.append(False)\n",
    "            pos_noun.append(0)\n",
    "            pos_tag.append(idx_tag)\n",
    "            idx += 1\n",
    "            \n",
    "        idx_tag += 1\n",
    "    \n",
    "    noun_phrases = [[idx2tag_.index(tag) for tag in templ[start : end]] for start, end, _ in hits]\n",
    "    positions = [idx for idx, tag in enumerate(tags) if tag == len(idx2tag_)]\n",
    "    assert sum(isnoun) == sum(map(len, noun_phrases))\n",
    "\n",
    "    return tags, noun_phrases, positions, isnoun, pos_noun, pos_tag\n",
    "\n",
    "rdd_np = rdd.map(_mapper).cache()\n",
    "dat['len_tag'] = collect(rdd_np.map(at(0)).map(len))\n",
    "dat['seq_tag'] = collect(rdd_np.flatMap(at(0)))\n",
    "dat['len_np'] = collect(rdd_np.map(at(1)).map(len))\n",
    "rdd_np_ = rdd_np.flatMap(at(1)).cache()\n",
    "dat['len_noun'] = collect(rdd_np_.map(len))\n",
    "dat['seq_noun'] = flat_collect(rdd_np_)\n",
    "dat['pos_np'] = collect(rdd_np.flatMap(at(2)))\n",
    "dat['isnoun'] = collect(rdd_np.flatMap(at(3)))\n",
    "dat['pos_noun'] = collect(rdd_np.flatMap(at(4)))\n",
    "dat['pos_tag'] = collect(rdd_np.flatMap(at(5)))\n",
    "np.savez(f'{output_dir}/data', **dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "0\n",
      "1\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([10, 30, 19, 30, 0, 2, 1],\n",
       " [[20, 8, 16, 18], [20]],\n",
       " [1, 3],\n",
       " [False, True, True, True, True, False, True, False, False, False],\n",
       " [0, 0, 1, 2, 3, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 5, 0, 7, 8, 9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mapper(rdd.take(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_np"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
