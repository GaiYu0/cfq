{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "import json\n",
    "from operator import *\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import matplotlib.pylab as pl\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/data/yu_gai/cfq'\n",
    "output_dir = '/work/yu_gai/cfq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlCtx.read.parquet(f'{input_dir}/dataset.parquet').sort('index').persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['complexityMeasures',\n",
       " 'expectedResponse',\n",
       " 'expectedResponseWithMids',\n",
       " 'index',\n",
       " 'question',\n",
       " 'questionPatternModEntities',\n",
       " 'questionTemplate',\n",
       " 'questionWithBrackets',\n",
       " 'questionWithMids',\n",
       " 'ruleIds',\n",
       " 'ruleTree',\n",
       " 'sparql',\n",
       " 'sparqlPattern',\n",
       " 'sparqlPatternModEntities']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcd1 95743 11968 11968 239357\n",
      "mcd2 95743 11968 11968 239357\n",
      "mcd3 95743 11968 11968 239357\n",
      "query_complexity_split 100654 9512 9512 239357\n",
      "query_pattern_split 94600 12489 12589 239357\n",
      "question_complexity_split 98999 10339 10340 239357\n",
      "question_pattern_split 95654 12115 11909 239357\n",
      "random_split 95744 11967 11967 239357\n"
     ]
    }
   ],
   "source": [
    "split_ids = !ls {input_dir}/splits | grep json\n",
    "for split_id in [s.replace('.json', '') for s in split_ids]:\n",
    "    split = json.load(open(f'{input_dir}/splits/{split_id}.json'))\n",
    "    np.savez(f'{output_dir}/splits/{split_id}', **{k : np.array(v) for k, v in split.items()})\n",
    "    print(split_id, len(split['trainIdxs']), len(split['devIdxs']), len(split['testIdxs']), df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = r\"(:?[a-zA-Z]+|M[0-9]|'s|,)\"\n",
    "p = re.compile(fr'(:?{w} )+{w}')\n",
    "df.rdd.map(lambda r: re.match(p, r['questionPatternModEntities']).string).zip(df.rdd.map(lambda r: r['questionPatternModEntities'])).map(lambda r: r[0] == r[1]).reduce(and_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace(q):\n",
    "    for s in [\n",
    "        'art director',\n",
    "        'country of nationality',\n",
    "        'costume designer',\n",
    "        'executive producer',\n",
    "        'executive produce',\n",
    "        'executive produced',\n",
    "        'film director',\n",
    "        'film distributor',\n",
    "        'film editor',\n",
    "        'film producer',\n",
    "        'production company',\n",
    "    ]:\n",
    "        q = q.replace(s, s.replace(' ', ''))\n",
    "    return q\n",
    "\n",
    "df = df.withColumn('questionPatternModEntities', udf(replace, StringType())('questionPatternModEntities')).persist()\n",
    "df.rdd.map(lambda r: len(r['questionPatternModEntities'].split(' ')) == len(r['questionTemplate'].split(' '))).reduce(and_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roles = df.rdd.flatMap(lambda r: re.findall(r'\\[[^\\]]+\\]', r['questionTemplate'])).distinct().collect()\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = '(?:%s)' % '|'.join(fr'\\[{role[1 : -1]}\\]' for role in roles)\n",
    "p = re.compile(fr'{r} and {r}|(?:{r} , )+and {r}')\n",
    "\n",
    "def find_grps(t):\n",
    "    toks = t.split(' ')\n",
    "    lens = np.array(list(map(len, toks)))\n",
    "    ends = np.cumsum(lens) + np.arange(len(toks))\n",
    "    starts = ends - lens\n",
    "\n",
    "    if re.search(p, t) is None:\n",
    "        grps = [[i] for i in range(len(toks))]\n",
    "        return grps, grps\n",
    "    \n",
    "    m_start, m_end = zip(*([m.start(), m.end()] for m in re.finditer(p, t)))\n",
    "    hit = False\n",
    "    grps = []\n",
    "    for idx, [start, end] in enumerate(zip(starts, ends)):\n",
    "        if start in m_start:\n",
    "            hit = True\n",
    "            grps.append([])\n",
    "        if hit:\n",
    "            grps[-1].append(idx)\n",
    "        else:\n",
    "            grps.append([idx])\n",
    "        if end in m_end:\n",
    "            hit = False\n",
    "    \n",
    "    for start, end, grp in zip(m_start, m_end, (grp for grp in grps if len(grp) > 1)):\n",
    "        assert t[start : end] == ' '.join(toks[idx] for idx in grp)\n",
    "    \n",
    "    grp2pos = [grp if len(grp) == 1 else [idx for idx in grp if toks[idx] in roles] for grp in grps]\n",
    "\n",
    "    return grps, grp2pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "get = lambda rdd, i: rdd.map(lambda r: r[i])\n",
    "collect = lambda rdd: np.array(rdd.collect())\n",
    "fcollect = lambda rdd: np.array(rdd.flatMap(lambda r: r).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP = '[SEP]'\n",
    "PAD = '[PAD]'\n",
    "def mapper(r):\n",
    "    toks = r['questionPatternModEntities'].split(' ')\n",
    "    entities = sorted(set(re.findall(r'M[0-0]', r['questionPatternModEntities'])))\n",
    "    variables = sorted(set(re.findall(r'\\?x[0-9]', r['sparqlPatternModEntities'])))\n",
    "    concepts = []\n",
    "    for line in r['sparqlPatternModEntities'].split('\\n')[1 : -1]:\n",
    "        if 'FILTER' not in line:\n",
    "            [[concept, *_]] = re.findall(r'^[^ ]+ [^ ]+ ([^ ]+)( .)?$', line)\n",
    "            if concept.startswith('ns:'):\n",
    "                concepts.append(concept)\n",
    "    concepts = sorted(set(concepts))\n",
    "    tail = [SEP] + concepts + [SEP] + variables\n",
    "    seq = toks + tail\n",
    "    isconcept = len(toks) * [False] + len(concepts) * [True] + len(variables) * [False]\n",
    "    isvariable = len(toks) * [False] + len(concepts) * [False] + len(variables) * [True]    \n",
    "    grps, grp2pos = find_grps(r['questionTemplate'] + ' ' + ' '.join(tail))\n",
    "    pos2grp = list(itertools.chain(len(grp) * [idx] for idx, grp in enumerate(grps)))\n",
    "    return seq, isconcept, isvariable, grp2pos, pos2grp\n",
    "\n",
    "rdd = df.rdd.map(mapper).cache()\n",
    "seq_rdd = get(rdd, 0).cache()\n",
    "idx2tok = sorted(seq_rdd.flatMap(lambda r: r).distinct().collect() + [PAD])\n",
    "tok2idx = dict(map(reversed, enumerate(idx2tok)))\n",
    "\n",
    "d['n_tok'] = collect(seq_rdd.map(len))\n",
    "d['seq'] = fcollect(seq_rdd.map(lambda r: [tok2idx[tok] for tok in r]))\n",
    "d['isconcept'], d['isvariable'] = fcollect(get(rdd, 1)), fcollect(get(rdd, 2))\n",
    "pos2grp = get(rdd, 3).cache()\n",
    "d['n_grp'] = collect(grp2pos.map(len))\n",
    "d['pos2grp'] = collect(get(rdd, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = sorted(tok for tok in idx2tok if re.match(r'^M[0-9]$', tok))\n",
    "concepts = sorted(tok for tok in idx2tok if tok.startswith('ns:'))\n",
    "variables = sorted(tok for tok in idx2tok if re.match(r'^\\?x[0-9]$', tok))\n",
    "sp_toks = sc.broadcast(set(entities + concepts + variables))\n",
    "def mapper(r):\n",
    "    d = defaultdict(list)\n",
    "    for i, tok in enumerate(r):\n",
    "        if tok in sp_toks.value:\n",
    "            d[tok].append(i)\n",
    "\n",
    "    n = len(d)\n",
    "    tok = [tok2idx[tok] for tok in sorted(d)]\n",
    "    n_idx = [len(d[k]) for k in sorted(d)]\n",
    "    idx = sum((d[k] for k in sorted(d)), [])\n",
    "\n",
    "    return n, tok, n_idx, idx\n",
    "\n",
    "rdd = seq_rdd.map(mapper).cache()\n",
    "d['n'] = collect(get(rdd, 0))\n",
    "d['tok'] = fcollect(get(rdd, 1))\n",
    "d['n_idx'] = fcollect(get(rdd, 2))\n",
    "d['idx'] = fcollect(get(rdd, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_rel(r):\n",
    "    src, rel, dst = [], [], []\n",
    "    for line in r['sparqlPatternModEntities'].split('\\n')[1 : -1]:\n",
    "        if 'FILTER' in line:\n",
    "            [[src_, dst_, *_]] = re.findall(r'^FILTER \\( ([^ ]+) != ([^ ]+) \\)( .)?$', line)\n",
    "            src.append(src_)\n",
    "            rel.append('!=')\n",
    "            dst.append(dst_)\n",
    "        else:\n",
    "            [[src_, rel_, dst_, *_]] = re.findall(r'^([^ ]+) ([^ ]+) ([^ ]+)( .)?$', line)\n",
    "            src.append(src_)\n",
    "            rel.append(rel_)\n",
    "            dst.append(dst_)\n",
    "\n",
    "    u, inv = np.unique(src + dst, return_inverse=True)\n",
    "    src, dst = np.split(np.arange(len(u))[inv], 2)\n",
    "    return src, rel, dst\n",
    "\n",
    "rdd = df.rdd.map(mapper_rel).cache()\n",
    "d['src'], d['dst'] = fcollect(get(rdd, 0)), fcollect(get(rdd, 2))\n",
    "rel_rdd = get(rdd, 1).cache()\n",
    "d['m'] = collect(rel_rdd.map(len))\n",
    "rel_rdd = rel_rdd.flatMap(lambda r: r).cache()\n",
    "idx2rel = sorted(rel_rdd.distinct().collect())\n",
    "rel2idx = {rel : idx for idx, rel in enumerate(idx2rel)}\n",
    "d['rel'] = collect(rel_rdd.map(lambda r: rel2idx[r]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([idx2tok, tok2idx], open(f'{output_dir}/tok-vocab.pickle', 'wb'))\n",
    "pickle.dump([idx2rel, rel2idx], open(f'{output_dir}/rel-vocab.pickle', 'wb'))\n",
    "np.savez(f'{output_dir}/data', **d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum multiplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapper(r):\n",
    "    src, _, dst = r\n",
    "    _, c = np.unique(np.vstack([src, dst]), return_counts=True, axis=1)\n",
    "    return c.max()\n",
    "\n",
    "rdd.map(mapper).reduce(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "tok_rdd = df.rdd.map(lambda r: [tok2idx[tok] for tok in r['questionPatternModEntities'].split(' ')])\n",
    "d['seq'] = fcollect(tok_rdd)\n",
    "d['n_tok'] = collect(tok_rdd.map(len))\n",
    "d['n_var'] = collect(df.rdd.map(lambda r: len(set(re.findall(r'\\?x[0-9]', r['sparqlPatternModEntities'])))))\n",
    "np.savez(f'{output_dir}/nvar', **d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(r):\n",
    "    concepts = []\n",
    "    for line in r['sparqlPatternModEntities'].split('\\n')[1 : -1]:\n",
    "        if 'FILTER' not in line:\n",
    "            [[concept, *_]] = re.findall(r'^[^ ]+ [^ ]+ ([^ ]+)( .)?$', line)\n",
    "            if concept.startswith('ns:'):\n",
    "                concepts.append(concept)\n",
    "    return set(concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "tok_rdd = df.rdd.map(lambda r: [tok2idx[tok] for tok in r['questionPatternModEntities'].split(' ')])\n",
    "d['seq'] = fcollect(tok_rdd)\n",
    "d['n_tok'] = collect(tok_rdd.map(len))\n",
    "\n",
    "con = df.rdd.map(mapper).cache()\n",
    "d['n_con'] = collect(con.map(len))\n",
    "idx2con = sorted(con.flatMap(lambda r: r).distinct().collect())\n",
    "con2idx = {con : idx for idx, con in enumerate(idx2con)}\n",
    "d['con'] = fcollect(con.map(lambda r: sorted(con2idx[con] for con in r)))\n",
    "\n",
    "np.savez(f'{output_dir}/concept', **d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
