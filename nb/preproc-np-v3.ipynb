{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import *\n",
    "import json\n",
    "from operator import *\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import matplotlib.pylab as pl\n",
    "import nltk\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['complexityMeasures',\n",
       " 'expectedResponse',\n",
       " 'expectedResponseWithMids',\n",
       " 'index',\n",
       " 'question',\n",
       " 'questionPatternModEntities',\n",
       " 'questionTemplate',\n",
       " 'questionWithBrackets',\n",
       " 'questionWithMids',\n",
       " 'ruleIds',\n",
       " 'ruleTree',\n",
       " 'sparql',\n",
       " 'sparqlPattern',\n",
       " 'sparqlPatternModEntities']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir = '/data/yu_gai/cfq'\n",
    "output_dir = '/work/yu_gai/cfq/data/cfq'\n",
    "\n",
    "df = sqlCtx.read.parquet(f'{input_dir}/dataset.parquet').sort('index').persist()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcd1 95743 11968 11968 239357\n",
      "mcd2 95743 11968 11968 239357\n",
      "mcd3 95743 11968 11968 239357\n",
      "query_complexity_split 100654 9512 9512 239357\n",
      "query_pattern_split 94600 12489 12589 239357\n",
      "question_complexity_split 98999 10339 10340 239357\n",
      "question_pattern_split 95654 12115 11909 239357\n",
      "random_split 95744 11967 11967 239357\n"
     ]
    }
   ],
   "source": [
    "splits = {}\n",
    "split_ids = !ls {input_dir}/splits | grep json\n",
    "for split_id in [s.replace('.json', '') for s in split_ids]:\n",
    "    split = splits[split_id] = json.load(open(f'{input_dir}/splits/{split_id}.json'))\n",
    "    np.savez(f'{output_dir}/splits/{split_id}', **{k : np.array(v) for k, v in split.items()})\n",
    "    print(split_id, len(split['trainIdxs']), len(split['devIdxs']), len(split['testIdxs']), df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace(q):\n",
    "    for s in [\n",
    "        'art director',\n",
    "        'country of nationality',\n",
    "        'costume designer',\n",
    "        'executive producer',\n",
    "        'executive produce',\n",
    "        'executive produced',\n",
    "        'film director',\n",
    "        'film distributor',\n",
    "        'film editor',\n",
    "        'film producer',\n",
    "        'production company',\n",
    "    ]:\n",
    "        q = q.replace(s, s.replace(' ', ''))\n",
    "    return q\n",
    "\n",
    "df = df.withColumn('questionPatternModEntities', udf(replace, StringType())('questionPatternModEntities')).persist()\n",
    "df.rdd.map(lambda r: len(r['questionPatternModEntities'].split(' ')) == len(r['questionTemplate'].split(' '))).reduce(and_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "at = lambda i: (lambda x: x[i])\n",
    "k1 = lambda r: [r, 1]\n",
    "unique = lambda rdd: sorted(rdd.distinct().collect())\n",
    "count = lambda rdd: dict(rdd.map(k1).reduceByKey(add).collect())\n",
    "collect = lambda rdd: np.array(rdd.collect())\n",
    "flat_collect = lambda rdd: np.array(rdd.flatMap(lambda r: r).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP, NIL = '{SEP}', '{NIL}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_vocab = idx2tok, tok2idx = pickle.load(open(f'{output_dir}/tok-vocab.pickle', 'rb'))\n",
    "tag_vocab = idx2tag, tag2idx = pickle.load(open(f'{output_dir}/tag-vocab.pickle', 'rb'))\n",
    "typ_vocab = idx2typ, typ2idx = pickle.load(open(f'{output_dir}/typ-vocab.pickle', 'rb'))\n",
    "idx2attr, _ = pickle.load(open(f'{output_dir}/attr-vocab.pickle', 'rb'))\n",
    "roles, _ = pickle.load(open(f'{output_dir}/role-vocab.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rel(line):\n",
    "    if 'FILTER' in line:\n",
    "        [[src, dst, *_]] = re.findall(r'^FILTER \\( ([^ ]+) != ([^ ]+) \\)( .)?$', line)\n",
    "        return src, '!=', dst\n",
    "    else:\n",
    "        [[src, typ, dst, *_]] = re.findall(r'^([^ ]+) ([^ ]+) ([^ ]+)( .)?$', line)\n",
    "        return src, typ, dst\n",
    "\n",
    "r = '(?:%s)' % '|'.join(fr'\\[{role[1 : -1]}\\]' for role in roles)  # TODO\n",
    "# p = re.compile(fr'{r} and {r}|(?:{r} , )+and {r}')\n",
    "p = re.compile('|'.join(fr'{r} and {r}|(?:{r} , )+and {r}' for r in [fr'\\[{role[1 : -1]}\\]' for role in roles]))\n",
    "def grp_by_tag(tags):\n",
    "    lens = np.array(list(map(len, tags)))\n",
    "    ends = np.cumsum(lens) + np.arange(len(tags))\n",
    "    starts = ends - lens\n",
    "\n",
    "    t = ' '.join(tags)\n",
    "    homo = lambda s: sum(role in s for role in roles) == 1\n",
    "    matches = [m for m in re.finditer(p, t) if homo(m.group())]\n",
    "    if not matches:\n",
    "        grps = [[i] for i in range(len(tags))]\n",
    "        return grps\n",
    "    \n",
    "    m_start, m_end = zip(*([m.start(), m.end()] for m in matches))\n",
    "    hit = False\n",
    "    grps = []\n",
    "    for idx, [start, end] in enumerate(zip(starts, ends)):\n",
    "        if start in m_start:\n",
    "            hit = True\n",
    "            grps.append([])\n",
    "        if hit:\n",
    "            grps[-1].append(idx)\n",
    "        else:\n",
    "            grps.append([idx])\n",
    "        if end in m_end:\n",
    "            hit = False\n",
    "    \n",
    "    for start, end, grp in zip(m_start, m_end, (grp for grp in grps if len(grp) > 1)):\n",
    "        assert t[start : end] == ' '.join(tags[idx] for idx in grp)\n",
    "\n",
    "    return grps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18378"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _mapper(r):\n",
    "    rels = list(map(find_rel, r['sparqlPatternModEntities'].split('\\n')[1 : -1]))\n",
    "    srcs, typs, dsts = zip(*rels)\n",
    "    ents = sorted({x for x in chain(srcs, dsts) if re.match('M\\d', x)})\n",
    "\n",
    "    toks = r['questionPatternModEntities'].split(' ')\n",
    "    tags = r['questionTemplate'].split(' ')\n",
    "    grps = grp_by_tag(tags)\n",
    "\n",
    "    return ' '.join(tags[idx] for idx, *_ in grps)\n",
    "\n",
    "df.rdd.map(_mapper).distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def _mapper(r):\n",
    "    rels = list(map(find_rel, r['sparqlPatternModEntities'].split('\\n')[1 : -1]))\n",
    "    srcs, typs, dsts = zip(*rels)\n",
    "    ents = sorted({x for x in chain(srcs, dsts) if re.match('M\\d', x)})\n",
    "\n",
    "    toks = r['questionPatternModEntities'].split(' ')\n",
    "    tags = r['questionTemplate'].split(' ')\n",
    "    grps = grp_by_tag(tags)\n",
    "    \n",
    "    templ = [tags[idx].replace('[', '').replace(']', '') for idx, *_ in grps]\n",
    "    starts, ends, _ = zip(*find_noun_phrases(templ))\n",
    "    idx, seq = 0, []\n",
    "    while idx < len(grps):\n",
    "        if idx in starts:\n",
    "            seq.append('NP_COMPLEX')\n",
    "            idx = ends[starts.index(idx)]\n",
    "        else:\n",
    "            seq.append(templ[idx])\n",
    "            idx += 1\n",
    "    \n",
    "    return r['index'], ' '.join(seq)\n",
    "\n",
    "templs = df.rdd.map(_mapper).cache()\n",
    "uniq_templs1, uniq_templs2, uniq_templs3 = [{k : set(templs.filter(lambda r: r[0] in v).map(at(1)).collect())\n",
    "                                             for k, v in splits[f'mcd{i}'].items()} for i in [1, 2, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mapper(r):\n",
    "    dat = {}\n",
    "    \n",
    "    rels = list(map(find_rel, r['sparqlPatternModEntities'].split('\\n')[1 : -1]))\n",
    "    srcs, typs, dsts = zip(*rels)\n",
    "    ents = sorted({x for x in chain(srcs, dsts) if re.match('M\\d', x) or re.match('\\?x\\d', x)})\n",
    "    seq_var = sorted(ent for ent in ents if ent.startswith('?x'))\n",
    "\n",
    "    toks = r['questionPatternModEntities'].split(' ') + [NIL] + seq_var\n",
    "    tags = r['questionTemplate'].split(' ') + [NIL] + seq_var\n",
    "    grps = grp_by_tag(tags)\n",
    "\n",
    "    dat['seq'] = [tag2idx[tags[idx]] for idx, *_ in grps]\n",
    "    dat['mem'] = [[tok2idx[toks[grp[0]]]] if len(grp) == 1 else\n",
    "                  [tok2idx[toks[idx]] for idx in grp if tags[idx] in roles] for grp in grps if not toks[grp[0]].startswith('?x')]\n",
    "    \n",
    "    ent2grp = {}\n",
    "    for idx, tok in zip(chain(*(len(grp) * [idx] for idx, grp in enumerate(grps))), toks):\n",
    "        if tok in ents:\n",
    "            ent2grp[tok] = idx\n",
    "    idx2grp = dat['idx2grp'] = sorted(set(ent2grp.values()))\n",
    "    ent2idx = {ent : idx2grp.index(ent2grp[ent]) for ent in ents}\n",
    "    \n",
    "    dat['rel'] = [[ent2idx[src], ent2idx[dst], idx2typ.index(typ)] for src, typ, dst in rels if typ in idx2typ]\n",
    "\n",
    "    return dat\n",
    "\n",
    "dat, rdd = {}, df.rdd.map(_mapper).cache()\n",
    "dat['n_rel'] = collect(rdd.map(at('rel')).map(len))\n",
    "rdd_rel = rdd.flatMap(at('rel')).cache()\n",
    "dat['src'], dat['dst'], dat['typ'] = [collect(rdd_rel.map(at(i))) for i in range(3)]\n",
    "dat['n_grp'] = collect(rdd.map(at('mem')).map(len))\n",
    "rdd_mem = rdd.flatMap(at('mem')).cache()\n",
    "dat['n_mem'], dat['mem'] = collect(rdd_mem.map(len)), flat_collect(rdd_mem)\n",
    "dat['n'] = collect(rdd.map(at('idx2grp')).map(len))\n",
    "dat['idx2grp'] = collect(rdd.flatMap(at('idx2grp')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5\n",
      "                             S               \n",
      "                             |                \n",
      "                             NP              \n",
      "            _________________|___________     \n",
      "           NP                       |    |   \n",
      "  _________|__________              |    |    \n",
      " |                    N             |    |   \n",
      " |          __________|______       |    |    \n",
      " DT        JJ                N      PP   NP  \n",
      " |         |                 |      |    |    \n",
      " a  ADJECTIVE_SIMPLE     NP_SIMPLE  of entity\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP\n",
    "    DT -> 'a' | 'an'\n",
    "    JJ -> 'ADJECTIVE_SIMPLE'\n",
    "    N -> 'NP_SIMPLE' | 'ROLE_SIMPLE' | JJ N\n",
    "    NP -> 'entity' | DT N | 'What' | 'What' N | 'Which' N | 'Who' | NP POS N | NP PP NP | NP 'whose' N\n",
    "    POS -> \"'s\"\n",
    "    PP -> 'of'\n",
    "\"\"\")\n",
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "def find_noun_phrases(tags):\n",
    "    hits = []\n",
    "    start = 0\n",
    "    while start < len(tags):\n",
    "        for end in range(len(tags), start, -1):\n",
    "            try:\n",
    "                trees = list(parser.parse(tags[start : end]))\n",
    "            except ValueError:\n",
    "                trees = []\n",
    "            if len(trees) > 0:\n",
    "                hits.append([start, end, trees])\n",
    "                start = end - 1\n",
    "                break\n",
    "        start += 1\n",
    "    return hits\n",
    "            \n",
    "for start, end, trees in find_noun_phrases(\"a ADJECTIVE_SIMPLE NP_SIMPLE of entity\".split(' ')):\n",
    "    print(start, end)\n",
    "    for tree in trees:\n",
    "        tree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['S',\n",
       "  'NP',\n",
       "  'NP',\n",
       "  'DT',\n",
       "  'a',\n",
       "  'N',\n",
       "  'JJ',\n",
       "  'ADJECTIVE_SIMPLE',\n",
       "  'N',\n",
       "  'NP_SIMPLE',\n",
       "  'PP',\n",
       "  'of',\n",
       "  'NP',\n",
       "  'entity'],\n",
       " [False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True,\n",
       "  False,\n",
       "  True],\n",
       " [[0, 1],\n",
       "  [1, 2],\n",
       "  [2, 3],\n",
       "  [3, 4],\n",
       "  [2, 5],\n",
       "  [5, 6],\n",
       "  [6, 7],\n",
       "  [5, 8],\n",
       "  [8, 9],\n",
       "  [1, 10],\n",
       "  [10, 11],\n",
       "  [1, 12],\n",
       "  [12, 13]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traverse(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mapper(r):\n",
    "    dat = {}\n",
    "    \n",
    "    idx2tag_ = [tag.replace('[', '').replace(']', '') for tag in idx2tag]\n",
    "    templ = [idx2tag_[idx] for idx in takewhile(lambda idx: not idx2tag[idx].startswith('?x'), r['seq'])]\n",
    "    hits = dat['hits'] = find_noun_phrases(templ)\n",
    "\n",
    "    dat['seq_noun'] = [r['seq'][start : end] for start, end, _ in hits]\n",
    "    \n",
    "    idx = 0\n",
    "    starts, ends, trees = zip(*hits)\n",
    "    seq_tag = dat['seq_tag'] = []\n",
    "    pos_np = dat['pos_np'] = []\n",
    "    pos_all = dat['pos_all'] = []\n",
    "    istag, isnoun, isvar = dat['istag'], dat['isnoun'], dat['isvar'] = [], [], []\n",
    "    while idx < len(templ):\n",
    "        if idx in starts:\n",
    "            start, end, _ = hits[starts.index(idx)]\n",
    "            disp = end - start\n",
    "\n",
    "            pos_np.append(len(seq_tag))\n",
    "            seq_tag.append(len(idx2tag_))\n",
    "            \n",
    "            pos_all.extend(range(disp))\n",
    "            istag.extend(disp * [False])\n",
    "            isnoun.extend(disp * [True])\n",
    "            isvar.extend(disp * [False])\n",
    "            \n",
    "            idx = end\n",
    "        else:\n",
    "            seq_tag.append(idx2tag_.index(templ[idx]))\n",
    "            \n",
    "            pos_all.append(len(seq_tag) - 1)\n",
    "            istag.append(True)\n",
    "            isnoun.append(False)\n",
    "            isvar.append(False)\n",
    "            \n",
    "            idx += 1\n",
    "            \n",
    "    seq_var = dat['seq_var'] = r['seq'][len(templ):] + [idx2tag.index('{NIL}')]   \n",
    "    pos_all.extend(range(len(seq_var)))\n",
    "    istag.extend(len(seq_var) * [False])\n",
    "    isnoun.extend(len(seq_var) * [False])\n",
    "    isvar.extend(len(seq_var) * [True])\n",
    "\n",
    "    return dat\n",
    "\n",
    "rdd_np = rdd.map(_mapper).cache()\n",
    "dat['len_tag'] = collect(rdd_np.map(at('seq_tag')).map(len))\n",
    "dat['seq_tag'] = collect(rdd_np.flatMap(at('seq_tag')))\n",
    "rdd_noun = rdd_np.flatMap(at('seq_noun')).cache()\n",
    "dat['len_noun'] = collect(rdd_noun.map(len))\n",
    "dat['seq_noun'] = flat_collect(rdd_noun)\n",
    "dat['len_var'] = collect(rdd_np.map(at('seq_var')).map(len))\n",
    "dat['seq_var'] = flat_collect(rdd_np.map(at('seq_var')))\n",
    "dat['n_np'] = collect(rdd_np.map(at('pos_np')).map(len))\n",
    "dat['pos_np'] = collect(rdd_np.flatMap(at('pos_np')))\n",
    "dat['n_all'] = collect(rdd_np.map(at('pos_all')).map(len))\n",
    "dat['pos_all'] = collect(rdd_np.flatMap(at('pos_all')))\n",
    "dat['istag'] = collect(rdd_np.flatMap(at('istag')))\n",
    "dat['isnoun'] = collect(rdd_np.flatMap(at('isnoun')))\n",
    "dat['isvar'] = collect(rdd_np.flatMap(at('isvar')))\n",
    "np.savez(f'{output_dir}/data', **dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"'s\",\n",
       "  'ADJECTIVE_SIMPLE',\n",
       "  'DT',\n",
       "  'JJ',\n",
       "  'N',\n",
       "  'NP',\n",
       "  'NP_SIMPLE',\n",
       "  'POS',\n",
       "  'PP',\n",
       "  'ROLE_SIMPLE',\n",
       "  'S',\n",
       "  'What',\n",
       "  'Which',\n",
       "  'Who',\n",
       "  'a',\n",
       "  'an',\n",
       "  'entity',\n",
       "  'of',\n",
       "  'whose'],\n",
       " {\"'s\": 0,\n",
       "  'ADJECTIVE_SIMPLE': 1,\n",
       "  'DT': 2,\n",
       "  'JJ': 3,\n",
       "  'N': 4,\n",
       "  'NP': 5,\n",
       "  'NP_SIMPLE': 6,\n",
       "  'POS': 7,\n",
       "  'PP': 8,\n",
       "  'ROLE_SIMPLE': 9,\n",
       "  'S': 10,\n",
       "  'What': 11,\n",
       "  'Which': 12,\n",
       "  'Who': 13,\n",
       "  'a': 14,\n",
       "  'an': 15,\n",
       "  'entity': 16,\n",
       "  'of': 17,\n",
       "  'whose': 18})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_lhs = set(str(p.lhs()) for p in grammar.productions())\n",
    "uniq_rhs = set(chain(*(map(str, p.rhs()) for p in grammar.productions())))\n",
    "idx2symb = sorted(uniq_lhs.union(uniq_rhs))\n",
    "symb2idx = {symb : idx for idx, symb in enumerate(idx2symb)}\n",
    "pickle.dump([idx2symb, symb2idx], open(f'{output_dir}/symb-vocab.pickle', 'wb'))\n",
    "idx2symb, symb2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(tree, nid=0):\n",
    "    labels, isleaf, edges = [tree.label()], [False], []\n",
    "    subtree_nid = nid + 1\n",
    "    for subtree in tree:\n",
    "        edges.append([nid, subtree_nid])\n",
    "        if isinstance(subtree, nltk.tree.Tree):\n",
    "            subtree_labels, subtree_isleaf, subtree_edges = traverse(subtree, subtree_nid)\n",
    "            subtree_nid += len(subtree_labels)\n",
    "            labels.extend(subtree_labels)\n",
    "            isleaf.extend(subtree_isleaf)\n",
    "            edges.extend(subtree_edges)\n",
    "        elif isinstance(subtree, str):\n",
    "            subtree_nid += 1\n",
    "            labels.append(subtree)\n",
    "            isleaf.append(True)\n",
    "        else:\n",
    "            raise TypeError()\n",
    "            \n",
    "    return labels, isleaf, edges\n",
    "\n",
    "def _mapper(r):\n",
    "    dat = {}\n",
    "    \n",
    "    dat['symb'], dat['isleaf'], dat['src'], dat['dst'] = [], [], [], []\n",
    "    for _, _, trees in r['hits']:\n",
    "        labels, isleaf, edges = traverse(trees[0])\n",
    "        dat['symb'].append([symb2idx[symb] for symb in labels])\n",
    "        dat['isleaf'].append(isleaf)\n",
    "        src, dst = zip(*edges)\n",
    "        dat['src'].append(src)\n",
    "        dat['dst'].append(dst)\n",
    "        \n",
    "    return dat\n",
    "\n",
    "rdd_tree = rdd_np.map(_mapper).cache()\n",
    "dat['n_tree'] = collect(rdd_tree.flatMap(at('symb')).map(len))\n",
    "dat['symb'] = flat_collect(rdd_tree.flatMap(at('symb')))\n",
    "dat['isleaf'] = flat_collect(rdd_tree.flatMap(at('isleaf')))\n",
    "dat['m_tree'] = collect(rdd_tree.flatMap(at('src')).map(len))\n",
    "dat['src_tree'] = flat_collect(rdd_tree.flatMap(at('src')))\n",
    "dat['dst_tree'] = flat_collect(rdd_tree.flatMap(at('dst')))\n",
    "np.savez(f'{output_dir}/data', **dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_tree.zip(rdd_np).filter(lambda r: any(sum(isleaf) != len(seq_noun) for isleaf, seq_noun in zip(r[0]['isleaf'], r[1]['seq_noun']))).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 624095}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count(rdd_tree.flatMap(lambda r: [len(symb) - max(dst) for symb, dst in zip(r['symb'], r['dst'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 624095}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count(rdd_tree.flatMap(lambda r: [len(symb) - max(src) for symb, src in zip(r['symb'], r['src'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(239357,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['m_tree'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'len_var': array([2, 2, 3, ..., 2, 2, 3]),\n",
       " 'seq_var': array([2, 1, 2, ..., 2, 3, 1]),\n",
       " 'n_rel': array([ 3,  7,  2, ..., 12, 12,  6]),\n",
       " 'src': array([2, 2, 2, ..., 1, 1, 2]),\n",
       " 'dst': array([0, 1, 1, ..., 2, 2, 0]),\n",
       " 'typ': array([ 3,  7, 24, ..., 24, 25, 17]),\n",
       " 'n_grp': array([ 9,  8, 11, ...,  5,  5,  9]),\n",
       " 'n_mem': array([1, 1, 1, ..., 1, 1, 1]),\n",
       " 'mem': array([14, 21,  8, ...,  1,  2,  3]),\n",
       " 'n': array([3, 3, 3, ..., 2, 2, 3]),\n",
       " 'idx2grp': array([1, 6, 8, ..., 5, 7, 8]),\n",
       " 'len_tag': array([5, 5, 5, ..., 4, 4, 4]),\n",
       " 'seq_tag': array([10, 30, 19, ..., 19, 30,  1]),\n",
       " 'len_noun': array([4, 1, 3, ..., 1, 1, 4]),\n",
       " 'seq_noun': array([20,  8, 16, ..., 18, 25, 20]),\n",
       " 'n_np': array([2, 2, 2, ..., 2, 2, 2]),\n",
       " 'pos_np': array([1, 3, 1, ..., 2, 0, 2]),\n",
       " 'n_all': array([ 9,  8, 11, ...,  5,  5,  9]),\n",
       " 'pos_all': array([0, 0, 1, ..., 6, 0, 1]),\n",
       " 'istag': array([ True, False, False, ...,  True, False, False]),\n",
       " 'isnoun': array([False,  True,  True, ..., False, False, False]),\n",
       " 'isvar': array([False, False, False, ..., False,  True,  True])}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239357"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2templ = lambda seq: ' '.join(idx2tag[idx] for idx in takewhile(lambda idx: idx2tag[idx] != '{SEP}', seq))\n",
    "def _mapper(r):\n",
    "    tags = seq2templ(r).replace('[', '').replace(']', '').split(' ')\n",
    "    hits = find_noun_phrases(tags)\n",
    "    return all(any(start <= idx < end for start, end, _ in hits)\n",
    "               for idx, tag in enumerate(tags) if tag in ['entity', 'NP_SIMPLE', 'ROLE_SIMPLE'])\n",
    "\n",
    "rdd.map(at(3)).map(_mapper).reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../cfq')\n",
    "\n",
    "from data import RaggedArray, CFQDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CFQDataset(splits['mcd1']['trainIdxs'], dat, tok_vocab, tag_vocab, typ_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Did [NP_COMPLEX] [VP_SIMPLE] [NP_COMPLEX] , [VP_SIMPLE] [NP_COMPLEX] , and [VP_SIMPLE] [NP_COMPLEX] {NIL}'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = dataset[0]\n",
    "' '.join(idx2tag[idx] if idx < len(idx2tag) else '[NP_COMPLEX]' for idx in dat['seq_tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['pos_all'][dat['isvar']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['pos_all'][dat['isnoun']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  2,  4,  5,  7,  8,  9, 11])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['pos_all'][dat['istag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Did   [VP_SIMPLE]   , [VP_SIMPLE]  , and [VP_SIMPLE]  {NIL} ?x0 ?x1 {NIL}'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = []\n",
    "for pos, istag, isnoun, isvar in zip(dat['pos_all'], dat['istag'], dat['isnoun'], dat['isvar']):\n",
    "    if istag:\n",
    "        seq.append(idx2tag[dat['seq_tag'][pos]])\n",
    "    elif isnoun:\n",
    "        seq.append('')\n",
    "    elif isvar:\n",
    "        seq.append(idx2tag[dat['seq_var'][pos]])\n",
    "    else:\n",
    "        raise RuntimeError()\n",
    "        \n",
    "' '.join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([21, 17]), array([21, 17]), array([20]), array([20])]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['seq_noun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['pos_all'][dat['isnoun']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([21, 17]), array([21, 17]), array([20]), array([20])]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['seq_noun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "templs = df.rdd.map(lambda r: r['questionTemplate']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Did a [NP_SIMPLE] [VP_SIMPLE] a [NP_SIMPLE] , [VP_SIMPLE] [entity] , and [VP_SIMPLE] [entity]'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templs[dat['index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 12, 14, 15,  8, 12, 14, 15,  8, 12, 14, 15,  8, 12, 14, 15])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True, False,  True,  True, False, False,  True,\n",
       "       False, False, False,  True, False, False, False, False])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
